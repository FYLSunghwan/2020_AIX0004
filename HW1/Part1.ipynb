{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI+X R-Py 중간과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.  본 질문에서는 뉴스를 키워드로 하여 뉴스 검색 첫번째 페이지에서 네이버 플랫폼에서포맷팅된 뉴스 링크 즉  “https://news.naver.com/main/read.nhn” 로 시작하는 모든 링크를 t수집해 보려고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def progress(i, total):\n",
    "    print(f\"{i+1}/{total}\\r\", end=\"\")\n",
    "    if i+1==total:\n",
    "        print(\"\\nFinished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = urllib.parse.quote(\"금리\")\n",
    "url = \"https://search.naver.com/search.naver?where=news&query=\"+ key_words+ \"&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2020.04.13&de=2020.04.14\"\n",
    "req = urllib.request.urlopen(url)\n",
    "data = req.read().decode('utf-8')\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "anchor_set = soup.findAll('a')\n",
    "news_link = []\n",
    "\n",
    "for item in anchor_set:\n",
    "    if re.search('^https://news.naver.com/main/read.nhn',item['href']):\n",
    "        news_link.append(item['href'])\n",
    "        \n",
    "count_tag = soup.find(\"div\", {\"class\", \"title_desc all_my\"})\n",
    "count_text = count_tag.find(\"span\").get_text().split()\n",
    "total_num = count_text[-1][0:-1].replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. 위에서 제시된 total_num을 이용해서 “금리” 검색 관련해서 전체 기사에 대해 네이버 플랫폼에서포맷팅된 뉴스 링크 즉  “https://news.naver.com/main/read.nhn” 로 시작하는 모든 링크를 찾아서 new_link 리스트에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "news_link = []\n",
    "key_words = urllib.parse.quote(\"금리\")\n",
    "for val in range(int(total_num)//10):\n",
    "    progress(val,int(total_num)//10)\n",
    "    start_val = str(10*val + 1)\n",
    "    url = \"https://search.naver.com/search.naver?where=news&query=\" + key_words + \"&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2020.04.13&de=2020.04.14&docid=&nso=so:r,p:from20191013to20191014,a:all&mynews=0&cluster_rank=26&start=\" + start_val + \"&refresh_start=0\"\n",
    "    req = urllib.request.urlopen(url)\n",
    "    data = req.read().decode('utf-8')\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    anchor_set = soup.findAll('a')\n",
    "    for item in anchor_set:\n",
    "        if re.search('^https://news.naver.com/main/read.nhn', item['href']):\n",
    "            if not item['href'] in news_link:\n",
    "                news_link.append(item['href'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. 위에서 구성한 news_link 리스트를 이용해서 뉴스 제목과 뉴스 본문 내용을 각각 title_list와 text_list로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417/417\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "title_list = []\n",
    "text_list = []\n",
    "\n",
    "for i, url in enumerate(news_link):\n",
    "    progress(i, len(news_link))\n",
    "    req = urllib.request.urlopen(url)\n",
    "    data = req.read()\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    title = soup.find(\"h3\", {\"id\": \"articleTitle\"}).get_text()\n",
    "    text = soup.find(\"div\", {\"id\": \"articleBodyContents\"}).get_text()\n",
    "    title_list.append(title)\n",
    "    text_list.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. title_list에 저장된 뉴스 제목에 대해 “금리”로 시작하고, 중간에 모든 문자가 여러 번 나와도 상관없으며, “인하”로 끝나는 문자열이 있는지 정규표현식을 활용하여 검색하고, 이 검색이 맞으면 제목을 출력하는 프로그램을 구현하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "용산구, 중소·소상공인·청년기업 융자금리 0%대로 인하\n",
      "용산구, 코로나 피해 중소상공인·청년기업 살린다…융자금리 0%대로 인하\n",
      "용산구, 중소·청년기업 융자금리 0%대로 인하\n",
      "자본硏 \"국채매입제도 도입·기준금리 추가 인하 필요\"\n",
      "자본시장연구원 “국채금리 안정화 위해 국채매입·금리인하 필요”\n",
      "[우정이야기]우체국보험 약관 대출 금리 인하\n",
      "5월 금리인하 전망 ‘솔솔’…“실효하한은 가변적”\n",
      "내달 금리인하 전망… 유동성 활용 가능성도\n",
      "\"한은, 이르면 5월 추가 금리인하…장기채 매수 유리\"\n"
     ]
    }
   ],
   "source": [
    "for title in title_list:\n",
    "    if re.search(\".*금리.*인하.*\", title):\n",
    "        print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
